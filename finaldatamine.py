# -*- coding: utf-8 -*-
"""finalDataMine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBcou1QrqButviIJ2OMDVn6188JMBTsR

# **Text Summarization**

---
https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/

รายชื่อสมาชิกกลุ่ม
1. ฉัตริน เหมรา 6242017426
2. บรรณวิชญ์ เอี่ยมสวัสดิ์ 6242051726
3. ยศวัจน์ อู่สิริมณีชัย 6242079326
4. รมย์รดา ลาดมะโรง 6242080926
5. ศรณ์ พงษ์นริศร 6242089626
6. สพล เชวงโชติ 6242097626
7. คุณานนท์ วิมุตติไชย 6242011626

**Step 1: Preparing the data**

หากมีการดึงเอาบทความมาจากอินเทอร์เน็ตใช้ก็จะมีขั้นตอนดังนี้ ถ้ามีข้อมูลtextอยู่แล้วสามารถข้ามขั้นตอนแรกไปได้เลย
"""

import bs4 as BeautifulSoup
import urllib.request  

# Fetching the content from the URL
fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/20th_century')

article_read = fetched_data.read()

# Parsing the URL content and storing in a variable
article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')

# Returning <p> tags
paragraphs = article_parsed.find_all('p')

article_content = ''

# Looping through the paragraphs and adding them to the variable
for p in paragraphs:  
    article_content += p.text

print(article_content)

"""**Step 2: Processing the data**

เตรียมข้อมูลเพื่อสร้างdictionaryที่รวมคำที่สำคัญและคะแนนของคำนั้นๆ
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

def _create_dictionary_table(text_string) -> dict:
   
    # Removing stop words
    stop_words = set(stopwords.words("english"))
    
    #split into tokens or words
    words = word_tokenize(text_string)
    
    # Reducing words to their root form
    stem = PorterStemmer()
    
    # Creating dictionary for the word frequency table
    frequency_table = dict()
    for wd in words:
        wd = stem.stem(wd)
        if wd in stop_words:
            continue
        if wd in frequency_table:
            frequency_table[wd] += 1
        else:
            frequency_table[wd] = 1

    return frequency_table

def check_max_frequency(frequency_table) -> int:
  max_frequency = 0
  for wd in frequency_table:
    if frequency_table[wd] > max_frequency:
      max_frequency = frequency_table[wd]
    
  return max_frequency

def _create_weight_dictionary_table(text_string, max_frequency) -> dict:
  # Removing stop words
    stop_words = set(stopwords.words("english"))
    
    #split into tokens or words
    words = word_tokenize(text_string)
    
    # Reducing words to their root form
    stem = PorterStemmer()
    
    # Creating dictionary for the word frequency table
    frequency_table = dict()
    for wd in words:
        wd = stem.stem(wd)
        if wd in stop_words:
            continue
        if wd in frequency_table:
            frequency_table[wd] += 1 / max_frequency
        else:
            frequency_table[wd] = 1 / max_frequency

    return frequency_table

print(stopwords.words("english"))

stem = PorterStemmer()
examples = ['wants', 'hated', 'eating', 'died', 'see', 'went', 'dying']
stem_examples = [stem.stem(example) for example in examples]
print(stem_examples)

frequency_table = _create_dictionary_table(article_content)

print(frequency_table)

max_frequency = check_max_frequency(frequency_table)
print(max_frequency)

weight_frequency_table = _create_weight_dictionary_table(article_content, max_frequency)
print(weight_frequency_table)

check_max_weight = check_max_frequency(weight_frequency_table)
print(check_max_weight)

"""**Step 3:  Tokenizing the article into sentences**

ทำการแบ่งเนื้อหาเป็นประโยคแต่ละประโยค แล้วเก็บใส่เข้าตัวแปร sentences โดยใช้ built-in function จาก nltk คือ sent_tokenize
"""

from nltk.tokenize import sent_tokenize

sentences = sent_tokenize(article_content)

print(sentences)

"""**Step 4: Finding the weighted frequencies of the sentences**

ทำอัลกอริทึมหาค่าน้ำหนักของแต่ละประโยค เพื่อหาว่าประโยคใดเป็นใจความสำคัญของบทความ
และเพื่อป้องกันไม่ให้ประโยคที่ยาวมีค่าน้ำหนักมากกว่าประโยคที่สั้น เราจึงต้องหารค่าน้ำหนักด้วยจำนวนคำในประโยคโดยไม่นับรวม stop word ในประโยคนั้น ๆ
"""

def _calculate_sentence_scores(sentences, weight_frequency_table) -> dict:   

    # Algorithm for scoring a sentence by its words
    sentence_weight = dict()

    for sentence in sentences:
        sentence_wordcount = (len(word_tokenize(sentence)))
        sentence_wordcount_without_stop_words = 0
        for word_weight in weight_frequency_table:
            if word_weight in sentence.lower():
                sentence_wordcount_without_stop_words += 1
                if sentence[:7] in sentence_weight:
                    sentence_weight[sentence[:7]] += weight_frequency_table[word_weight]
                else:
                    sentence_weight[sentence[:7]] = weight_frequency_table[word_weight]

        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] /        sentence_wordcount_without_stop_words
      
    return sentence_weight

sentence_scores = _calculate_sentence_scores(sentences, weight_frequency_table)

print(sentence_scores)

max_weight_senetence = check_max_frequency(sentence_scores)
print(max_weight_senetence)

"""**Step 5: Calculating the threshold of the sentences**

สร้างเกณฑ์จากค่าเฉลี่ยของคะแนนแต่ละประโยค
"""

def _calculate_average_score(sentence_weight) -> int:
   
    # Calculating the average score for the sentences
    sum_values = 0
    for entry in sentence_weight:
        sum_values += sentence_weight[entry]

    # Getting sentence average value from source text
    average_score = (sum_values / len(sentence_weight))

    return average_score

threshold = _calculate_average_score(sentence_scores)

print(threshold)

"""**Step 6: Getting the summary**

สรุปบทความจากตัวแปรที่สร้าง
"""

def _get_article_summary(sentences, sentence_weight, threshold):
    sentence_counter = 0
    article_summary = ''

    for sentence in sentences:
        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):
            article_summary += " " + sentence
            sentence_counter += 1

    return article_summary

article_summary = _get_article_summary(sentences, sentence_scores, 1.5 * threshold)

print(article_summary)